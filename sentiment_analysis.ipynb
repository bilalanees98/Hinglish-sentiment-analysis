{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i170317_AI_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5oSTNB8Xz34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5219ad9-759e-4679-c513-d6c3a71eda5d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfEj_w0HaK0j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize the stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "hinglishStopList = []\n",
        "with open(\"hinglish_stopwords.txt\",'r') as reader:\n",
        "    for line in reader:\n",
        "        hinglishStopList.append(line.strip())\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKfIM4qYZLiR",
        "outputId": "fc121009-46e7-4df9-fca6-8a62af0dea41"
      },
      "source": [
        "print ((stoplist))\n",
        "print ((hinglishStopList))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWlnT86tdIP8"
      },
      "source": [
        "# --------------Clean Data code, creates sentences fron conll tokens, removes usernames and urls\n",
        "# --------------creates cleaned_train.tsv with cleaned_conll_to_tsv func\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "class ConllToken:\n",
        "    \"\"\" Parent token class. \"\"\"\n",
        "    def __init__(self, token_type):\n",
        "        self.token_type = token_type\n",
        "\n",
        "\n",
        "class Sentiment(ConllToken):\n",
        "    \"\"\" Sentiment token class. \"\"\"\n",
        "    def __init__(self, sentiment):\n",
        "        super().__init__(\"sentiment\")\n",
        "        self.sentiment = sentiment\n",
        "    \n",
        "    @staticmethod\n",
        "    def is_instance(token):\n",
        "        return token.token_type == \"sentiment\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (Sentiment.is_instance(other) and \n",
        "                other.sentiment == self.sentiment)\n",
        "\n",
        "\n",
        "class EndOfSegment(ConllToken):\n",
        "    \"\"\" End of Segment token class. \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\"end\")\n",
        "        \n",
        "    @staticmethod\n",
        "    def is_instance(token):\n",
        "        return token.token_type == \"end\"\n",
        "        \n",
        "    def __eq__(self, other):\n",
        "        return EndOfSegment.is_instance(other)\n",
        "\n",
        "\n",
        "class BasicToken(ConllToken):\n",
        "    \"\"\" Basic token class. \"\"\"\n",
        "    def __init__(self, value):\n",
        "        super().__init__(\"basic\")\n",
        "        self.value = value\n",
        "\n",
        "    @staticmethod\n",
        "    def is_instance(token):\n",
        "        return token.token_type == \"basic\"\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return (BasicToken.is_instance(other) and \n",
        "                other.value == self.value)\n",
        "\n",
        "\n",
        "class URL(ConllToken):\n",
        "    \"\"\" URL token class. \"\"\"\n",
        "    def __init__(self, value):\n",
        "        super().__init__(\"url\")\n",
        "        self.value = value\n",
        "\n",
        "    @staticmethod\n",
        "    def is_instance(token):\n",
        "        return token.token_type == \"url\"\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return URL.is_instance(other) and other.value == self.value\n",
        "\n",
        "\n",
        "class Username(ConllToken):\n",
        "    \"\"\" Username token class. \"\"\"\n",
        "    def __init__(self, value):\n",
        "        super().__init__(\"username\")\n",
        "        self.value = value\n",
        "\n",
        "    @staticmethod\n",
        "    def is_instance(token):\n",
        "        return token.token_type == \"username\"\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return Username.is_instance(other) and other.value == self.value\n",
        "\n",
        "\n",
        "def tokenize_conll(lines, sentimentNotAvailable):\n",
        "    \"\"\" Tokenize lines in a file. \"\"\"\n",
        "    for line in lines:\n",
        "        # print (line)\n",
        "        if line.strip() == \"\":\n",
        "            # print ('triggered')\n",
        "            yield EndOfSegment()\n",
        "        else:\n",
        "            fields = line.split(\"\\t\")\n",
        "            if fields[0] == \"meta\":\n",
        "                if sentimentNotAvailable:\n",
        "                  yield Sentiment(\"none\")\n",
        "                else:\n",
        "                  yield Sentiment(fields[2].strip())\n",
        "            else:\n",
        "                yield BasicToken(fields[0].strip())\n",
        "\n",
        "\n",
        "def cluster_urls(tokens):\n",
        "    \"\"\" \n",
        "    Generator for piecing together URLs in token streams.\n",
        "    Reads from an instream, and if the instream has broken URL pieces\n",
        "    (identified by finding 'http') in conll format, the generator yields \n",
        "    a complete URL.\n",
        "    \"\"\"\n",
        "    url_builder = \"\"\n",
        "    for token in tokens:\n",
        "        if BasicToken.is_instance(token):\n",
        "            if url_builder == \"\" and not token.value.startswith(\"http\"):\n",
        "                yield token\n",
        "            elif token.value.startswith(\"http\"):                \n",
        "                url_builder = token.value\n",
        "            else:\n",
        "                url_builder += token.value\n",
        "        else:\n",
        "            if url_builder != \"\":\n",
        "                yield URL(url_builder)\n",
        "                url_builder = \"\"\n",
        "            yield token\n",
        "    if url_builder != \"\":\n",
        "        yield URL(url_builder) \n",
        "\n",
        "\n",
        "def cluster_usernames(tokens):\n",
        "    \"\"\" \n",
        "    Generator for piecing together Twitter usernames in token streams.\n",
        "    Reads from an instream, and if the instream has broken username pieces\n",
        "    (identified by an '@' symbol) in conll format, the generator \n",
        "    yields a complete username.\n",
        "    \"\"\"\n",
        "    builder = \"\"\n",
        "    for token in tokens:\n",
        "        if BasicToken.is_instance(token) and builder == \"\":\n",
        "            if token.value == \"@\":\n",
        "                builder = token.value\n",
        "            else:\n",
        "                yield token\n",
        "        elif BasicToken.is_instance(token):            \n",
        "            if builder == \"@\" or builder[-1] == \"_\":\n",
        "                builder += token.value\n",
        "            elif token.value == \"_\":\n",
        "                builder += token.value\n",
        "            else:\n",
        "                yield Username(builder)\n",
        "                builder = \"\"\n",
        "                if token.value == \"@\":\n",
        "                    builder = token.value\n",
        "                else:\n",
        "                    yield token\n",
        "        else:\n",
        "            if builder != \"\":\n",
        "                yield Username(builder)\n",
        "                builder = \"\"\n",
        "            yield token\n",
        "    if builder != \"\":\n",
        "        yield Username(builder) \n",
        "\n",
        "def conll_to_tsv(conll_file, tsv_file, sentimentNotAvailable = False):\n",
        "    \"\"\" Convert conll format to TSV. \"\"\"\n",
        "    testLabels=[]\n",
        "    if sentimentNotAvailable:\n",
        "      testLabels = read_test_labels(\"test_labels_hinglish.txt\")\n",
        "\n",
        "    with open(conll_file) as reader:\n",
        "        with open(tsv_file, 'w') as writer:\n",
        "\n",
        "            table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "            tokens = tokenize_conll([line for line in reader],sentimentNotAvailable)\n",
        "            tokens = cluster_urls(tokens)\n",
        "            tokens = cluster_usernames(tokens)\n",
        "            segment_tokens = []\n",
        "            sentiment = -1\n",
        "            for tok in tokens:\n",
        "                if Sentiment.is_instance(tok):\n",
        "                    if tok.sentiment == \"negative\":\n",
        "                        sentiment = 0\n",
        "                    elif tok.sentiment == \"neutral\":\n",
        "                        sentiment = 1\n",
        "                    elif tok.sentiment == \"positive\":\n",
        "                        sentiment = 2\n",
        "                    elif tok.sentiment == \"none\":\n",
        "                        sentiment = 3\n",
        "                elif EndOfSegment.is_instance(tok):                    \n",
        "                    next_segment = ' '.join(segment_tokens)\n",
        "                    next_segment += '\\t' + str(sentiment)\n",
        "                    if sentiment >= 0:\n",
        "                        writer.write(next_segment + '\\n')\n",
        "                    sentiment = -1\n",
        "                    segment_tokens = []\n",
        "                elif BasicToken.is_instance(tok):\n",
        "                    segment_tokens.append(tok.value)\n",
        "            if sentiment >= 0:\n",
        "                next_segment = ' '.join(segment_tokens)\n",
        "                next_segment += '\\t' + str(sentiment)\n",
        "                writer.write(next_segment + '\\n')\n",
        "    if sentimentNotAvailable:\n",
        "      write_test_labels(tsv_file, 'newDev.tsv',testLabels)\n",
        "\n",
        "def read_test_labels(input_file):\n",
        "  '''for reading from the hindi_test_unlablled file'''\n",
        "  testLabels = []\n",
        "  with open(input_file, 'r') as reader:\n",
        "    for line in reader:\n",
        "      line = line.split(',')\n",
        "      if line[0] != \"Uid\":\n",
        "        testUids.append(line[0])\n",
        "        line = line[1]\n",
        "        testLabels.append(line)\n",
        "    # testLabels = testLabels[1:]\n",
        "    for i in range(len(testLabels)):\n",
        "      if testLabels[i].strip() == \"negative\":\n",
        "        testLabels[i] = 0\n",
        "      elif testLabels[i].strip() == \"neutral\":\n",
        "        testLabels[i] = 1\n",
        "      elif testLabels[i].strip() == \"positive\":\n",
        "        testLabels[i] = 2 \n",
        "  return testLabels\n",
        "\n",
        "def write_test_labels(read_file, write_file,testLabels):\n",
        "  '''for writing to dev file in bag of words folder'''\n",
        "  with open(read_file,'r') as reader:\n",
        "    with open(write_file, 'w') as writer:\n",
        "      i = 0\n",
        "      for line in reader:\n",
        "        tweet = line[:-3]\n",
        "        writer.write(str(tweet) + \"\\t\" + str(testLabels[i]) + \"\\n\")\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def testify(input_file, output_file):\n",
        "    \"\"\" Formats given trial data into test data. \"\"\"\n",
        "    with open(input_file, 'r') as reader:\n",
        "        with open(output_file, 'w') as writer:\n",
        "            writer.write(\"index\\tsentence\\n\")\n",
        "            index = 0\n",
        "            for line in reader:\n",
        "                line = line.split(\"\\t\")[0]\n",
        "                if line != \"sentence\":\n",
        "                    writer.write(f\"{index}\\t{line}\\n\")\n",
        "                    index += 1\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "  # # drive/MyDrive/data/train_14k_split_conll.txt\n",
        "  #   conll_to_tsv('drive/MyDrive/data/train_14k_split_conll.txt','drive/MyDrive/data/bag-of-words/train.tsv',False)\n",
        "  #   # testify('drive/MyDrive/data/Hindi_test_unalbelled_conll_updated.txt', 'drive/MyDrive/data/bag-of-words/dev.tsv')\n",
        "  #   conll_to_tsv('drive/MyDrive/data/Hindi_test_unalbelled_conll_updated.txt','drive/MyDrive/data/bag-of-words/dev.tsv',True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18rXbJUfLVvn"
      },
      "source": [
        "\"\"\" Script containing functions and neural networks for the bag-of-words and count-words models. \"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def simple_accuracy(preds, labelss):\n",
        "  \n",
        "    # return np.equal(preds, labels).mean()\n",
        "    # f1_score(labels, preds, average = 'weighted')\n",
        "    # return metrics.confusion_matrix(y_true, y_pred), metrics.classification_report(y_true, y_pred, digits=3)\n",
        "    return np.equal(preds, labelss).mean(),metrics.f1_score(labelss,preds, average = 'weighted')\n",
        "\n",
        "def get_labels(filename):\n",
        "    \"\"\" Returns a list of labels for sentences in a given file. \"\"\"\n",
        "    with open(filename,'r') as reader:\n",
        "        labels = []\n",
        "        for line in reader:\n",
        "            line_split = line.split('\\t')\n",
        "            if line_split[1].strip() != 'label' and len(line_split)>1:\n",
        "                labels.append(int(line_split[1]))\n",
        "            elif len(line_split) < 1:\n",
        "                labels.append(0)\n",
        "        return labels\n",
        "\n",
        "def create_vocab(frequencies, K):\n",
        "    \"\"\" Creates a list of words as vocabulary by selecting words with a frequency > K. \"\"\"\n",
        "    if '…' in frequencies:\n",
        "        del frequencies['…']\n",
        "\n",
        "    for key in list(frequencies):\n",
        "        if frequencies[key] < K:\n",
        "            del frequencies[key]\n",
        "    return list(frequencies)\n",
        "\n",
        "\n",
        "\"\"\" Models for setting baseline. \"\"\" \n",
        "\n",
        "def two_layer_net(input_size, H):\n",
        "    \"\"\"\n",
        "    A two-layer feedforward neural network with 'input_size' input features, H hidden\n",
        "    features, and a softmax response value.\n",
        "    \n",
        "    \"\"\"\n",
        "    net = torch.nn.Sequential()\n",
        "    net.add_module(\"dense1\", torch.nn.Linear(in_features = input_size, \n",
        "                                   out_features = H))\n",
        "    net.add_module(\"relu1\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense2\", torch.nn.Linear(in_features = H, \n",
        "                                   out_features = 3))\n",
        "    net.add_module(\"softmax\", torch.nn.Softmax(dim=1))\n",
        "\n",
        "    return net.cuda()\n",
        "\n",
        "def three_layer_net(input_size, H1, H2):\n",
        "    \"\"\"\n",
        "    A three-layer feedforward neural network with 'input_size' input features, H1, H2 hidden\n",
        "    features, and a softmax response value.\n",
        "    \n",
        "    \"\"\"\n",
        "    net = torch.nn.Sequential()\n",
        "    net.add_module(\"dense1\", torch.nn.Linear(in_features = input_size, \n",
        "                                   out_features = H1))\n",
        "    net.add_module(\"relu1\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense2\", torch.nn.Linear(in_features = H1,\n",
        "                                   out_features = H2))\n",
        "    net.add_module(\"relu2\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense3\", torch.nn.Linear(in_features = H2,\n",
        "                                   out_features = 3))\n",
        "    net.add_module(\"softmax\", torch.nn.Softmax(dim=1))\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def four_layer_net(input_size, H1, H2, H3):\n",
        "    \"\"\"\n",
        "    A four-layer feedforward neural network with 'input_size' input features, H1, H2, H3 hidden\n",
        "    features, and a softmax response value.\n",
        "    \n",
        "    \"\"\"\n",
        "    net = torch.nn.Sequential()\n",
        "    net.add_module(\"dense1\", torch.nn.Linear(in_features = input_size, \n",
        "                                   out_features = H1))\n",
        "    net.add_module(\"relu1\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense2\", torch.nn.Linear(in_features = H1,\n",
        "                                   out_features = H2))\n",
        "    net.add_module(\"relu2\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense3\", torch.nn.Linear(in_features = H2,\n",
        "                                   out_features = H3))\n",
        "    net.add_module(\"relu3\", torch.nn.ReLU())\n",
        "    net.add_module(\"dense4\", torch.nn.Linear(in_features = H3,\n",
        "                                   out_features = 3))\n",
        "    net.add_module(\"softmax\", torch.nn.Softmax(dim=1))\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "def get_bigram_frequencies(filename):\n",
        "    \"\"\" Returns a dictionary of the frequencies of all words and all bigrams in a given file. \"\"\"\n",
        "    with open(filename,'r') as reader:\n",
        "        frequencies = {}\n",
        "        for line in reader:\n",
        "            words_in_line = line.split('\\t')[0].strip().split(' ')\n",
        "            for word in words_in_line:\n",
        "                if word in frequencies:\n",
        "                    frequencies[word] += 1\n",
        "                else:\n",
        "                    frequencies[word] = 1\n",
        "            # Make BiGrams\n",
        "            for i in range(1,len(words_in_line),2):\n",
        "                if i != len(words_in_line) - 1:\n",
        "                    bigram1 = words_in_line[i-1] + \" \" + words_in_line[i]\n",
        "                    if bigram1 in frequencies:\n",
        "                        frequencies[bigram1] += 1\n",
        "                    else:\n",
        "                        frequencies[bigram1] = 1\n",
        "                    bigram2 = words_in_line[i] + \" \" +  words_in_line[i+1]\n",
        "                    if bigram2 in frequencies:\n",
        "                        frequencies[bigram2] += 1\n",
        "                    else:\n",
        "                        frequencies[bigram2] = 1\n",
        "        return frequencies\n",
        "\n",
        "\n",
        "def create_bigram_vectors(filename, vocab):\n",
        "    \"\"\" Creates 1-hot vectors from a given vocabulary set of words. \"\"\"\n",
        "    tweet_vectors = []\n",
        "    hidden_length = len(vocab)\n",
        "    with open(filename,'r') as reader:\n",
        "        for line in reader:\n",
        "            tweet_tensor = torch.zeros(hidden_length)\n",
        "            line = line.split('\\t')[0].strip()\n",
        "            if line != 'sentence':\n",
        "                for i in range(0,hidden_length):\n",
        "                    if vocab[i] in line:\n",
        "                        tweet_tensor[i] = 1\n",
        "                tweet_vectors.append(tweet_tensor)\n",
        "        return tweet_vectors\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYmOFnqfoOrn"
      },
      "source": [
        "\"\"\" The script to train and run the Bag-of-Words BiGrams extension on a trainset and devset. \"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "##########################################################################################\n",
        "# Important Constants\n",
        "\n",
        "FREQ_THRESHOLD = 15\n",
        "BATCH_SIZE = 6\n",
        "VOCAB = []\n",
        "INPUT_SIZE = 0\n",
        "H1 = 300\n",
        "H2 = 180\n",
        "H3 = 75\n",
        "NUM_EPOCHS = 40\n",
        "LEMMATIZE = False\n",
        "STEM = False\n",
        "CLEANFILE = True\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "def remove_stopwords_punctutation(row):\n",
        "    tweet = str(row['sentences'])\n",
        "    tokens = word_tokenize(tweet)\n",
        "    # taken only words (not punctuation)\n",
        "    # and w not in hinglishStopList\n",
        "    # and w not in stoplist\n",
        "    token_words = [w for w in tokens if w.isalpha()and w not in stoplist]\n",
        "    return ' '.join(token_words)\n",
        "\n",
        "def clean_file(input_filename,output_filename , lemmatize, stem):\n",
        "  # will always remove punctuation and emojis, stemming and lemmatizing depends on parameters\n",
        "  testDf = pd.read_csv(input_filename, sep=\"\\t\", header=None, names = ['sentences','label'])\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  # reduce to lower case\n",
        "  testDf['sentences'] = testDf['sentences'].str.lower()\n",
        "  # remove some punctuation and stopwords\n",
        "  testDf['sentences'] = testDf.apply(remove_stopwords_punctutation,axis=1)\n",
        "\n",
        "  if lemmatize:\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    testDf['cleaned'] = testDf['sentences'].apply(lambda x: str(' '.join(tokenizer.tokenize(str(x))))).apply(lambda x: lemmatizer.lemmatize(x))\n",
        "  elif stem:\n",
        "    ps = PorterStemmer() \n",
        "    testDf['cleaned'] = testDf['sentences'].apply(lambda x: str(' '.join(tokenizer.tokenize(str(x))))).apply(lambda x: ps.stem(x))\n",
        "  else:\n",
        "    testDf['cleaned'] = testDf['sentences'].apply(lambda x: str(' '.join(tokenizer.tokenize(str(x)))))\n",
        "  # display(testDf)\n",
        "  testDf.to_csv(output_filename,columns = ['cleaned','label'],sep = '\\t',header = False, index = False)\n",
        "\n",
        "def make_train_vectors(input_filename):\n",
        "\n",
        "    # Create vectors\n",
        "    frequencies = get_bigram_frequencies(input_filename)\n",
        "    global VOCAB, INPUT_SIZE\n",
        "    VOCAB = create_vocab(frequencies, FREQ_THRESHOLD)\n",
        "    vecs = create_bigram_vectors(input_filename, VOCAB)\n",
        "\n",
        "    # Set value for input size\n",
        "    INPUT_SIZE = len(VOCAB)\n",
        "\n",
        "    # Get labels from file\n",
        "    labels = get_labels(input_filename)\n",
        "\n",
        "    assert(len(labels)==len(vecs))\n",
        "\n",
        "    return vecs, labels\n",
        "\n",
        "def make_test_vectors(input_filename):\n",
        "\n",
        "    # Create vectors\n",
        "    vecs = create_bigram_vectors(input_filename, VOCAB)\n",
        "\n",
        "    # Get labels from file\n",
        "    labels = get_labels(input_filename)#read_test_labels(\"drive/MyDrive/data/test_labels_hinglish.txt\")#get_labels(input_filename)\n",
        "\n",
        "    assert(len(labels)==len(vecs))\n",
        "\n",
        "    return vecs, labels\n",
        "\n",
        "class BiGramsTrainDataSet(Dataset):\n",
        "    \"\"\" The BiGrams training dataset containing sentence vectors and actual labels. \"\"\"\n",
        "    def __init__(self, datafile):\n",
        "        self.vecs, self.labels = make_train_vectors(datafile)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vecs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.vecs[idx],self.labels[idx]]\n",
        "\n",
        "class BiGramsTestDataSet(Dataset):\n",
        "    \"\"\" The BiGrams testing dataset containing sentence vectors and actual labels. \"\"\"\n",
        "    def __init__(self, datafile):\n",
        "        self.vecs, self.labels = make_test_vectors(datafile)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vecs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.vecs[idx],self.labels[idx]]\n",
        "\n",
        "\n",
        "def train(trainloader, model, criterion, optimizer):\n",
        "    \"\"\" Trains a model on a trainset. \"\"\"\n",
        "    best_model = model\n",
        "    best_so_far = -1\n",
        "    for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 1000))\n",
        "                running_loss = 0.0\n",
        "        \n",
        "        acc,f1 = eval(testloader,net,'outs.tsv','preds.tsv',testset.labels)\n",
        "        accuracyGlobal.append(acc)\n",
        "        f1ScoreGlobal.append(f1)\n",
        "        if f1 > best_so_far:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_so_far = f1\n",
        "    \n",
        "    print('Finished Training')\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def eval(testloader, net, outfile, labelsfile, actual_labels):\n",
        "    \"\"\" \n",
        "    Evaluates the results of trained model on a testset and writes results to files:\n",
        "        1. outfile: Contains all the output softmax tensors (for analysis)\n",
        "        2. labelsfile: Contains all predicted labels (for analysis)\n",
        "        3. data/bag-of-words/results.txt: Contains the accuracy on the testset.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outs = []\n",
        "        preds = []\n",
        "        for data in testloader:\n",
        "            vectors, _ = data\n",
        "            vectors = vectors.cuda()\n",
        "\n",
        "            outputs = net(vectors)\n",
        "            # get the predicted labels\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Add data to lists\n",
        "            outs.append(outputs)\n",
        "            preds.append(predicted)\n",
        "\n",
        "    # WRITE PREDICTED LABELS TO LABELSFILE\n",
        "        with open(labelsfile,'w') as label_writer:\n",
        "          with open(\"answer.txt\",'w') as answer_writer:\n",
        "            label_writer.write(\"index\\tlabel\\n\")\n",
        "            answer_writer.write(\"Uid,Sentiment\\n\")\n",
        "            # Each batch is of the shape: (BATCH_SIZE * 1)\n",
        "            # Ex: BATCH_SIZE = 4\n",
        "            #   [[x],\n",
        "            #   [x],\n",
        "            #   [x],\n",
        "            #   [x]]\n",
        "    \n",
        "            # Keep track of no. of sentences\n",
        "            counter = 0\n",
        "            for batch in range(len(preds)):\n",
        "                for idx in range(len(preds[batch])):\n",
        "                    # print (len(preds))\n",
        "                    # print (len(preds[batch]))\n",
        "                    predTowrite = \"\"\n",
        "                    temp = preds[batch][idx].item()\n",
        "                    if temp == 0:\n",
        "                      predToWrite = \"negative\"\n",
        "                    elif temp == 1:\n",
        "                      predToWrite = \"neutral\"\n",
        "                    elif temp == 2:\n",
        "                      predToWrite = \"positive\"\n",
        "                    label_writer.write(\"%s\\t%s\\n\" % (counter+1,temp))\n",
        "                    answer_writer.write(\"%s,%s\\n\" % (testUids[counter],predToWrite))\n",
        "                    counter += 1\n",
        "    \n",
        "    # WRITE OUTPUT SOFTMAX VECTORS TO OUTFILE    \n",
        "        with open(outfile,'w') as out_writer:\n",
        "            out_writer.write(\"index\\ttensor\\n\")\n",
        "            # Each batch is of the shape: (BATCH_SIZE * 3)\n",
        "            # Ex: BATCH_SIZE = 4\n",
        "            #   [[x,x,x],\n",
        "            #   [x,x,x],\n",
        "            #   [x,x,x],\n",
        "            #   [x,x,x]]\n",
        "\n",
        "            # Keep track of no. of sentences\n",
        "            counter = 0\n",
        "            for batch in range(len(outs)):\n",
        "                for idx in range(len(preds[batch])):\n",
        "                    out_writer.write(\"%s\\t%s\\n\" % (counter+1,outs[batch][idx]))\n",
        "                    counter += 1\n",
        "\n",
        "    # COMPUTE AND WRITE ACCURACY TO RESULTS.TXT\n",
        "        preds = get_labels(labelsfile)\n",
        "        accur,f1Score = simple_accuracy(preds,actual_labels)\n",
        "        with open('results.txt','w') as results:\n",
        "            results.write('acc = %s' % str(accur))\n",
        "            results.write('f1 score = %s' % str(f1Score))\n",
        "\n",
        "        print(\"Accuracy on test set: %s\" % str(accur))\n",
        "        print(\"f1 score on test set: %s\" % str(f1Score))\n",
        "\n",
        "    print(\"Finished Testing\")\n",
        "    return accur,f1Score\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEQ52ZRBbdS4",
        "outputId": "b8d2a6b0-732e-4de0-9afa-a4e1b477018e"
      },
      "source": [
        "import time\n",
        "if __name__ == \"__main__\": #### MAIN\n",
        "\n",
        "    start_time = time.clock()\n",
        "\n",
        "\n",
        "  # global uids for test data\n",
        "    testUids = []\n",
        "    # create train and test file for BOG \n",
        "    conll_to_tsv('train_14k_split_conll.txt','train.tsv',False)\n",
        "    conll_to_tsv('Hindi_test_unalbelled_conll_updated.txt','dev.tsv',True)\n",
        "\n",
        "    # clean both train and test file\n",
        "    if CLEANFILE:\n",
        "      clean_file('train.tsv','cleanTrain.tsv',LEMMATIZE, STEM)\n",
        "      clean_file('newDev.tsv','cleanDev.tsv',LEMMATIZE, STEM)\n",
        "\n",
        "      trainset = BiGramsTrainDataSet('cleanTrain.tsv')\n",
        "      testset = BiGramsTestDataSet('cleanDev.tsv')\n",
        "      \n",
        "    else:\n",
        "      trainset = BiGramsTrainDataSet('train.tsv')\n",
        "      testset = BiGramsTestDataSet('newDev.tsv')\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "    print (time.clock() - start_time, \"seconds\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.378368000000002 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rXAuHtfE7N5",
        "outputId": "d21bfd1b-ad82-45ee-89cd-fb560852b217"
      },
      "source": [
        "start_time = time.clock()\n",
        "# For 2 layer feedforward\n",
        "net = two_layer_net(INPUT_SIZE, H1)\n",
        "net = net.cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# for graphs\n",
        "accuracyGlobal = []\n",
        "f1ScoreGlobal = []\n",
        "\n",
        "print(\"Starting Training\\n\")\n",
        "# TRAIN!\n",
        "trained_model = train(trainloader, net, criterion, optimizer)\n",
        "\n",
        "print(\"Starting Testing\\n\")\n",
        "\n",
        "# TEST!\n",
        "eval(testloader,trained_model,'outs.tsv','preds.tsv',testset.labels)\n",
        "print (time.clock() - start_time, \"second\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training\n",
            "\n",
            "[1,  1000] loss: 1.093\n",
            "[1,  2000] loss: 1.081\n",
            "Accuracy on test set: 0.5346666666666666\n",
            "f1 score on test set: 0.5234932550249926\n",
            "Finished Testing\n",
            "[2,  1000] loss: 1.045\n",
            "[2,  2000] loss: 1.022\n",
            "Accuracy on test set: 0.5666666666666667\n",
            "f1 score on test set: 0.5615895372333545\n",
            "Finished Testing\n",
            "[3,  1000] loss: 0.996\n",
            "[3,  2000] loss: 0.984\n",
            "Accuracy on test set: 0.606\n",
            "f1 score on test set: 0.6036736834309151\n",
            "Finished Testing\n",
            "[4,  1000] loss: 0.966\n",
            "[4,  2000] loss: 0.969\n",
            "Accuracy on test set: 0.598\n",
            "f1 score on test set: 0.6002707533549679\n",
            "Finished Testing\n",
            "[5,  1000] loss: 0.953\n",
            "[5,  2000] loss: 0.946\n",
            "Accuracy on test set: 0.6213333333333333\n",
            "f1 score on test set: 0.6185907839952823\n",
            "Finished Testing\n",
            "[6,  1000] loss: 0.938\n",
            "[6,  2000] loss: 0.937\n",
            "Accuracy on test set: 0.6303333333333333\n",
            "f1 score on test set: 0.631690602136974\n",
            "Finished Testing\n",
            "[7,  1000] loss: 0.928\n",
            "[7,  2000] loss: 0.928\n",
            "Accuracy on test set: 0.6313333333333333\n",
            "f1 score on test set: 0.634141806182647\n",
            "Finished Testing\n",
            "[8,  1000] loss: 0.921\n",
            "[8,  2000] loss: 0.920\n",
            "Accuracy on test set: 0.6346666666666667\n",
            "f1 score on test set: 0.6370236166352664\n",
            "Finished Testing\n",
            "[9,  1000] loss: 0.916\n",
            "[9,  2000] loss: 0.916\n",
            "Accuracy on test set: 0.641\n",
            "f1 score on test set: 0.640222126804314\n",
            "Finished Testing\n",
            "[10,  1000] loss: 0.908\n",
            "[10,  2000] loss: 0.906\n",
            "Accuracy on test set: 0.6426666666666667\n",
            "f1 score on test set: 0.6420122935142416\n",
            "Finished Testing\n",
            "[11,  1000] loss: 0.902\n",
            "[11,  2000] loss: 0.903\n",
            "Accuracy on test set: 0.6476666666666666\n",
            "f1 score on test set: 0.6512585274097982\n",
            "Finished Testing\n",
            "[12,  1000] loss: 0.896\n",
            "[12,  2000] loss: 0.899\n",
            "Accuracy on test set: 0.6463333333333333\n",
            "f1 score on test set: 0.6484640430476298\n",
            "Finished Testing\n",
            "[13,  1000] loss: 0.886\n",
            "[13,  2000] loss: 0.892\n",
            "Accuracy on test set: 0.651\n",
            "f1 score on test set: 0.6526283910231065\n",
            "Finished Testing\n",
            "[14,  1000] loss: 0.887\n",
            "[14,  2000] loss: 0.891\n",
            "Accuracy on test set: 0.6546666666666666\n",
            "f1 score on test set: 0.6574215293133844\n",
            "Finished Testing\n",
            "[15,  1000] loss: 0.885\n",
            "[15,  2000] loss: 0.880\n",
            "Accuracy on test set: 0.6463333333333333\n",
            "f1 score on test set: 0.6428075595812879\n",
            "Finished Testing\n",
            "[16,  1000] loss: 0.875\n",
            "[16,  2000] loss: 0.880\n",
            "Accuracy on test set: 0.6596666666666666\n",
            "f1 score on test set: 0.6618927515922989\n",
            "Finished Testing\n",
            "[17,  1000] loss: 0.875\n",
            "[17,  2000] loss: 0.873\n",
            "Accuracy on test set: 0.656\n",
            "f1 score on test set: 0.6595493544938819\n",
            "Finished Testing\n",
            "[18,  1000] loss: 0.870\n",
            "[18,  2000] loss: 0.874\n",
            "Accuracy on test set: 0.6626666666666666\n",
            "f1 score on test set: 0.6653157543244422\n",
            "Finished Testing\n",
            "[19,  1000] loss: 0.869\n",
            "[19,  2000] loss: 0.870\n",
            "Accuracy on test set: 0.6556666666666666\n",
            "f1 score on test set: 0.6584418857740879\n",
            "Finished Testing\n",
            "[20,  1000] loss: 0.858\n",
            "[20,  2000] loss: 0.874\n",
            "Accuracy on test set: 0.654\n",
            "f1 score on test set: 0.6509157630521565\n",
            "Finished Testing\n",
            "[21,  1000] loss: 0.856\n",
            "[21,  2000] loss: 0.869\n",
            "Accuracy on test set: 0.6586666666666666\n",
            "f1 score on test set: 0.6611985503147939\n",
            "Finished Testing\n",
            "[22,  1000] loss: 0.861\n",
            "[22,  2000] loss: 0.858\n",
            "Accuracy on test set: 0.6576666666666666\n",
            "f1 score on test set: 0.6549535506003182\n",
            "Finished Testing\n",
            "[23,  1000] loss: 0.857\n",
            "[23,  2000] loss: 0.857\n",
            "Accuracy on test set: 0.6533333333333333\n",
            "f1 score on test set: 0.653951670027883\n",
            "Finished Testing\n",
            "[24,  1000] loss: 0.856\n",
            "[24,  2000] loss: 0.847\n",
            "Accuracy on test set: 0.663\n",
            "f1 score on test set: 0.6623945488316161\n",
            "Finished Testing\n",
            "[25,  1000] loss: 0.846\n",
            "[25,  2000] loss: 0.857\n",
            "Accuracy on test set: 0.6596666666666666\n",
            "f1 score on test set: 0.6617923466093315\n",
            "Finished Testing\n",
            "[26,  1000] loss: 0.847\n",
            "[26,  2000] loss: 0.841\n",
            "Accuracy on test set: 0.6506666666666666\n",
            "f1 score on test set: 0.6420949580929899\n",
            "Finished Testing\n",
            "[27,  1000] loss: 0.844\n",
            "[27,  2000] loss: 0.844\n",
            "Accuracy on test set: 0.6553333333333333\n",
            "f1 score on test set: 0.6522639638007828\n",
            "Finished Testing\n",
            "[28,  1000] loss: 0.840\n",
            "[28,  2000] loss: 0.842\n",
            "Accuracy on test set: 0.6603333333333333\n",
            "f1 score on test set: 0.6619209200971284\n",
            "Finished Testing\n",
            "[29,  1000] loss: 0.837\n",
            "[29,  2000] loss: 0.843\n",
            "Accuracy on test set: 0.6613333333333333\n",
            "f1 score on test set: 0.6607286804682161\n",
            "Finished Testing\n",
            "[30,  1000] loss: 0.833\n",
            "[30,  2000] loss: 0.834\n",
            "Accuracy on test set: 0.6603333333333333\n",
            "f1 score on test set: 0.6616241740953903\n",
            "Finished Testing\n",
            "[31,  1000] loss: 0.831\n",
            "[31,  2000] loss: 0.833\n",
            "Accuracy on test set: 0.6543333333333333\n",
            "f1 score on test set: 0.6577290273575619\n",
            "Finished Testing\n",
            "[32,  1000] loss: 0.835\n",
            "[32,  2000] loss: 0.828\n",
            "Accuracy on test set: 0.656\n",
            "f1 score on test set: 0.654900615344972\n",
            "Finished Testing\n",
            "[33,  1000] loss: 0.832\n",
            "[33,  2000] loss: 0.825\n",
            "Accuracy on test set: 0.653\n",
            "f1 score on test set: 0.6502105640338081\n",
            "Finished Testing\n",
            "[34,  1000] loss: 0.826\n",
            "[34,  2000] loss: 0.826\n",
            "Accuracy on test set: 0.6586666666666666\n",
            "f1 score on test set: 0.6587781489859328\n",
            "Finished Testing\n",
            "[35,  1000] loss: 0.822\n",
            "[35,  2000] loss: 0.823\n",
            "Accuracy on test set: 0.659\n",
            "f1 score on test set: 0.6607102150895219\n",
            "Finished Testing\n",
            "[36,  1000] loss: 0.818\n",
            "[36,  2000] loss: 0.823\n",
            "Accuracy on test set: 0.6496666666666666\n",
            "f1 score on test set: 0.6502944746748592\n",
            "Finished Testing\n",
            "[37,  1000] loss: 0.816\n",
            "[37,  2000] loss: 0.815\n",
            "Accuracy on test set: 0.6503333333333333\n",
            "f1 score on test set: 0.6527356563279406\n",
            "Finished Testing\n",
            "[38,  1000] loss: 0.820\n",
            "[38,  2000] loss: 0.812\n",
            "Accuracy on test set: 0.6573333333333333\n",
            "f1 score on test set: 0.657710170176964\n",
            "Finished Testing\n",
            "[39,  1000] loss: 0.816\n",
            "[39,  2000] loss: 0.809\n",
            "Accuracy on test set: 0.654\n",
            "f1 score on test set: 0.6567542661590202\n",
            "Finished Testing\n",
            "[40,  1000] loss: 0.807\n",
            "[40,  2000] loss: 0.811\n",
            "Accuracy on test set: 0.6626666666666666\n",
            "f1 score on test set: 0.6642631515865576\n",
            "Finished Testing\n",
            "Finished Training\n",
            "Starting Testing\n",
            "\n",
            "Accuracy on test set: 0.6626666666666666\n",
            "f1 score on test set: 0.6653157543244422\n",
            "Finished Testing\n",
            "208.050984 second\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGyHtp-r2gmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3a1fb127-a140-44ee-c2da-35c1cee56e00"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# loss_train = history.history['accuracy']\n",
        "# loss_val = history.history['f1 score']\n",
        "epochs = range(0,40)\n",
        "plt.plot(epochs, accuracyGlobal, 'g', label='accuracy')\n",
        "plt.plot(epochs, f1ScoreGlobal, 'b', label='f1 score')\n",
        "plt.title('Accuracy and f1 score')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('%')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re1Fk2Is3TkR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}